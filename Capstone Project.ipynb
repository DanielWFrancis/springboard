{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Capstone Project - Predicting Tags on a Corpus of Research Papers\n",
    "\n",
    "The Mercatus Center at George Mason University is a university research center that provides economic research and education. Mercatus employs a fairly large staff of media and policy outreach professionals who must digest and communicate economic findings to media outlets and political staffers in Washington, DC. \n",
    "\n",
    "Part of this outreach is the maintainance of a website that provides full access to all research papers, testimonials, opinion editorials, summaries, and other outputs. These publications are given tags that allow users to easily find related publications. So far, Mercatus has assigned tags somewhat haphazardly, entrusting junior staffers to choose tags with a preference for preexisting labels. \n",
    "\n",
    "The goal of this project is to clean the current corpus of tagged documents and then, using machine learning techniques available from the scikit-learn Python library, produce a machine learning model that can predict the most relevant tags for a new document. \n",
    "\n",
    "I have written a scraper that has crawled the Mercatus website for documents and saved each one, along with some useful metadata. The following notebook preprocesses this data and then fits a machine learning model that can be pickled and used later to produce actionable tag suggestions for Mercatus outreach staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First I use sklearn.dataset's `load_files()` method to load a classified dataset - a directory whose subdirectories' names are label names, and whose subdirectories' contents are the text files for that label. \n",
    "\n",
    "I then initialize and fill a dataframe from attributes of the load_files object, and drop rows which contain missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 1901 unique documents, with 2.850078905839032 labels per document, for a total of 5418 documents spread over 136 labels.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "trainer = load_files('new_data')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['filename'] = trainer.filenames\n",
    "df['text'] = [' '.join(open(f, 'r').read().split()) for f in trainer.filenames]\n",
    "df['label'] = [trainer.target_names[x] for x in trainer.target]\n",
    "df['author'] = [x.split('__')[0].split('--')[0].split('/')[-1] for x in df['filename']]\n",
    "df['date'] = pd.to_datetime([x.split('__')[-1].split('-')[-1].split('.txt')[0].strip() for x in df['filename']], \n",
    "                            errors='coerce')\n",
    "\n",
    "# Find documents with less than 250 characters and remove them\n",
    "df = df[pd.Series([len(x) for x in df.text]) > 250]\n",
    "\n",
    "# remove documents in NA values - sometimes dates got cut off,\n",
    "# which causes problems in the next cell\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Our dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"{len(df)/len(df.groupby('text').count())} labels per document, for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our dataset contains a large number of labels, and some have not been used in many years, while others have only been used a few times. \n",
    "\n",
    "The following code converts the `date` column to datetime objects, then groups by `label` and `date` to find only labels that have been active in the past four years (1460 days). \n",
    "\n",
    "We then remove labels that contain less than six documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our refined dataset contains 1873 unique documents, with for a total of 4985 documents spread over 91 labels.\n"
     ]
    }
   ],
   "source": [
    "df['date'] = df['date'].astype(np.int64) // 10 ** 9\n",
    "df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "\n",
    "df_active_labels = df[['label','date']].groupby('label').max().sort_values('date',ascending=True).reset_index()\n",
    "df_active_labels = df_active_labels[df_active_labels.date > datetime.date.today() - datetime.timedelta(1460)]\n",
    "df = df[df['label'].isin(df_active_labels.label)]\n",
    "\n",
    "df_large_categories = df.groupby('label').count()[df.groupby('label').count()['filename'] > 5].reset_index()[['label','filename']]\n",
    "df = df[df['label'].isin(df_large_categories.label)]\n",
    "\n",
    "print(f\"Our refined dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start working on our data. First we encode our labels and split our training and validation documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df.label)\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df.text.tolist(), y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we initialize two functions, one for stemming tokens using the `SnowballStemmer`, and the second for actually tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    tokens = [i for i in tokens if all(j.isalpha() or j in string.punctuation for j in i)]\n",
    "    tokens = [i for i in tokens if '/' not in i]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I initialize two vectorizers - a CountVectorizer and TfidfVectorizer, both of which cap the 1-gram and 2-gram features at 200,000 and impose a minimum document frequency of three and a maximum document frequency of 70% of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    cv = pickle.load(open(\"countvectorizer.pkl\", \"rb\"))\n",
    "    xtrain_cv = pickle.load(open(\"xtrain_cv.pkl\", \"rb\"))\n",
    "    xvalid_cv = pickle.load(open(\"xvalid_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    cv = CountVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "    cv.fit(list(set(xtrain)) + list(set(xvalid)))\n",
    "    xtrain_cv = cv.transform(xtrain)\n",
    "    xvalid_cv = cv.transform(xvalid)\n",
    "\n",
    "    pickle.dump(xtrain_cv, open(\"xtrain_cv.pkl\",\"wb\"))\n",
    "    pickle.dump(xvalid_cv, open(\"xvalid_cv.pkl\",\"wb\"))\n",
    "    pickle.dump(cv, open(\"countvectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    tfv = pickle.load(open(\"tfidfvectorizer.pkl\", \"rb\"))\n",
    "    xtrain_tfv = pickle.load(open(\"xtrain_tfv.pkl\", \"rb\"))\n",
    "    xvalid_tfv = pickle.load(open(\"xvalid_tfv.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    tfv = TfidfVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = 'english')\n",
    "\n",
    "    tfv.fit(list(xtrain) + list(xvalid))\n",
    "    xtrain_tfv =  tfv.transform(xtrain) \n",
    "    xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "    pickle.dump(xtrain_tfv, open(\"xtrain_tfv.pkl\",\"wb\"))\n",
    "    pickle.dump(xvalid_tfv, open(\"xvalid_tfv.pkl\",\"wb\"))\n",
    "    pickle.dump(tfv, open(\"tfidfvectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Tests: First Pass\n",
    "\n",
    "The goal is to provide users with the most relevant tags for a particular document. To accomplish this, I use `GridSearchCV` to tune hyperparameters for a variety of models, and then choose the best one. These models include `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, `KNeighborsClassifier`, and `LinearSVC`. These models will be tried using both tf-idf and count vectorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### LogisticRegression\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -3.3094250726587253 found with the following parameters: {'C': 5, 'dual': False, 'n_jobs': -1, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Testing LogisticRegression on tf-idf vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_tf = pickle.load(open(\"gscv_lr_tf.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    lr_tf = LogisticRegression()\n",
    "    params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_tf = GridSearchCV(lr_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_tf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "    pickle.dump(gscv_lr_tf, open(\"gscv_lr_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# Display LogisticRegression/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_tf.best_score_} found with the following parameters:',\n",
    "      gscv_lr_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/Users/dfrancis/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -9.599575059251444 found with the following parameters: {'C': 0.5, 'dual': False, 'n_jobs': -1, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# # Testing LogisticRegression on count vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_cv = pickle.load(open(\"gscv_lr_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    lr_cv = LogisticRegression()\n",
    "    params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_cv = GridSearchCV(lr_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_lr_cv, open(\"gscv_lr_cv.pkl\",\"wb\"))\n",
    "\n",
    "    \n",
    "# Display LogisticRegression/countvectorizer results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_cv.best_score_} found with the following parameters:',\n",
    "      gscv_lr_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### DecisionTreeClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -30.866439479022038 found with the following parameters: {'criterion': 'gini', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Testing DecisionTreeClassifier on tf-idf vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_dtc_tf = pickle.load(open(\"gscv_dtc_tf.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    dtc_tf = DecisionTreeClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_dtc_tf = GridSearchCV(dtc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_dtc_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_dtc_tf, open(\"gscv_dtc_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -30.50875466565311 found with the following parameters: {'criterion': 'entropy', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Testing DecisionTreeClassifier on count vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_dtc_cv = pickle.load(open(\"gscv_dtc_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    dtc_cv = DecisionTreeClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_dtc_cv = GridSearchCV(dtc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_dtc_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_dtc_cv, open(\"gscv_dtc_cv.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RandomForestClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -16.008716214283595 found with the following parameters: {'criterion': 'gini', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Testing RandomForestClassifier on tf-idf vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_rfc_tf = pickle.load(open(\"gscv_rfc_tf.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    rfc_tf = RandomForestClassifier()\n",
    "    params = {F6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_rfc_tf = GridSearchCV(rfc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_rfc_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_rfc_tf, open(\"gscv_rfc_tf.pkl\",\"wb\"))\n",
    "\n",
    "    \n",
    "# Display RandomForestClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -15.776711349094573 found with the following parameters: {'criterion': 'entropy', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Testing RandomForestClassifier on count vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_rfc_cv = pickle.load(open(\"gscv_rfc_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    rfc_cv = RandomForestClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_rfc_cv = GridSearchCV(rfc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_rfc_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_rfc_cv, open(\"gscv_rfc_cv.pkl\",\"wb\"))\n",
    "\n",
    "    \n",
    "# Display RandomForestClassifier/count results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -16.008716214283595 found with the following parameters: {'alpha': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Testing NaiveBayes on tf-idf vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_mnb_tf = pickle.load(open(\"gscv_mnb_tf.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    mnb_tf = MultinomialNB()\n",
    "    params = {'alpha':[0.01, 1, 3]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_mnb_tf = GridSearchCV(mnb_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_mnb_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_mnb_tf, open(\"gscv_mnb_tf.pkl\",\"wb\"))\n",
    "\n",
    "# Display NaiveBayes/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_mnb_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -15.776711349094573 found with the following parameters: {'alpha': 3}\n"
     ]
    }
   ],
   "source": [
    "# Testing NaiveBayes on count vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_mnb_cv = pickle.load(open(\"gscv_mnb_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    mnb_cv = MultinomialNB()\n",
    "    params = {'alpha':[0.01, 1, 3]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_mnb_cv = GridSearchCV(mnb_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_mnb_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_mnb_cv, open(\"gscv_mnb_cv.pkl\",\"wb\"))\n",
    "\n",
    "# Display NaiveBayes/count results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_mnb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Tests: A Second Pass\n",
    "\n",
    "As the test results show, LogisticRegression on the tf-idf vectorized data returns the most reliable results. A further dive into the the GridSearchCV test results shows a pattern of improving test scores as C-values increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.5, mean_test_score=-3.608504471830679\n",
      "For C=0.5, mean_test_score=-3.608512043830933\n",
      "For C=1.0, mean_test_score=-3.420553423569543\n",
      "For C=1.0, mean_test_score=-3.42056045461182\n",
      "For C=5.0, mean_test_score=-3.3094250726587253\n",
      "For C=5.0, mean_test_score=-3.309442896912445\n"
     ]
    }
   ],
   "source": [
    "for count, c in enumerate([0.5, 0.5, 1.0, 1.0, 5.0, 5.0]):\n",
    "    print(f\"For C={c}, mean_test_score={list(gscv_lr_tf.cv_results_['mean_test_score'])[count]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Exploring higher C-values makes sense, given the trend we are seeing here. Each `C` value appears twice as each value was tested along with a `dual` value of `True` and `False`; the `dual` value does not seem to make much of a difference, so we'll drop it in our next test and only test further values of `C`. We will also test a few other parameters, namely `penalty` and `tol`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -2.8691197965860096 found with the following parameters: {'C': 5, 'n_jobs': -1, 'penalty': 'l1', 'random_state': 42, 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# Second testing of LogisticRegression on tf-idf vectorized features.\n",
    "\n",
    "use_pickles = False\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_tf2 = pickle.load(open(\"gscv_mnb_tf.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    lr_tf2 = LogisticRegression()\n",
    "    params = {'penalty':['l1','l2'], 'C':[1, 5, 10], 'tol':[0.001, 0.0001, 0.00001], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_tf2 = GridSearchCV(lr_tf2, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_tf2.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_lr_tf2, open(\"gscv_lr_tf2.pkl\",\"wb\"))\n",
    "\n",
    "# Display LogisticRegression/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_tf2.best_score_} found with the following parameters:',\n",
    "      gscv_lr_tf2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Excellent! Reducing our log-loss to -2.87 is a big improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just to sate my curiosity, I used the `gscv_lr_tf2` model to run a test on a document that Mercatus has recently published that is not in the corpus, [this \"Economic Situation\" quarterly report](https://www.mercatus.org/publications/economic-situation-december-2017):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economics and Public Policy</td>\n",
       "      <td>0.121860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State and Local Policy</td>\n",
       "      <td>0.075437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federal Fiscal Policy</td>\n",
       "      <td>0.064857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Regulatory Process Reform</td>\n",
       "      <td>0.064778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regulation</td>\n",
       "      <td>0.051593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Disaster Recovery</td>\n",
       "      <td>0.049016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>0.048295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Financial Crisis</td>\n",
       "      <td>0.038694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Regulatory Analysis</td>\n",
       "      <td>0.031060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Development Economics</td>\n",
       "      <td>0.027822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Spending, Deficits, &amp; Debt</td>\n",
       "      <td>0.027345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Regulatory Accumulation</td>\n",
       "      <td>0.026811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Government Accountability</td>\n",
       "      <td>0.019214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Taxes</td>\n",
       "      <td>0.018034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>State and Local Budget Reform</td>\n",
       "      <td>0.016454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>State and Local Regulations</td>\n",
       "      <td>0.015459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Financial Regulation</td>\n",
       "      <td>0.014394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Workforce Participation</td>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Energy and Environmental Regulations</td>\n",
       "      <td>0.012615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Trade and Immigration</td>\n",
       "      <td>0.012492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Budget Process</td>\n",
       "      <td>0.010951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Study of American Capitalism</td>\n",
       "      <td>0.010662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Monetary Policy</td>\n",
       "      <td>0.009799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Economic History</td>\n",
       "      <td>0.009051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Barriers to Entry</td>\n",
       "      <td>0.008577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Food and Health Regulations</td>\n",
       "      <td>0.008462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>0.008214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Technology Policy</td>\n",
       "      <td>0.007377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>0.006928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Spending and Budget Reform</td>\n",
       "      <td>0.006685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Medicaid</td>\n",
       "      <td>0.002084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Healthcare Favoritism</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Online Privacy</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Cybersecurity</td>\n",
       "      <td>0.001887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Economic Mobility</td>\n",
       "      <td>0.001872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Immigration Reform</td>\n",
       "      <td>0.001766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Midnight Regulations</td>\n",
       "      <td>0.001737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>State and Local Tax Policy</td>\n",
       "      <td>0.001693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Department of Health and Human Services</td>\n",
       "      <td>0.001657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Net Neutrality</td>\n",
       "      <td>0.001643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Securities Regulation</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Regulatory Impact Analysis</td>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Medicare</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Economics of Culture</td>\n",
       "      <td>0.001474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Department of Energy</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Transportation Financing</td>\n",
       "      <td>0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Financial Stability</td>\n",
       "      <td>0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Income Inequality</td>\n",
       "      <td>0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Monetary Rules</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Education Policy</td>\n",
       "      <td>0.001022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Copyright and Intellectual Property</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Sharing Economy</td>\n",
       "      <td>0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Spectrum Reform</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Internet Freedom</td>\n",
       "      <td>0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Global Financial Markets</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>FinTech</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Export-Import Bank</td>\n",
       "      <td>0.000597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Balance of Trade</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Social Security Administration</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>0.000393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      label  probability\n",
       "0               Economics and Public Policy     0.121860\n",
       "1                    State and Local Policy     0.075437\n",
       "2                     Federal Fiscal Policy     0.064857\n",
       "3                 Regulatory Process Reform     0.064778\n",
       "4                                Regulation     0.051593\n",
       "5                         Disaster Recovery     0.049016\n",
       "6                         Financial Markets     0.048295\n",
       "7                          Financial Crisis     0.038694\n",
       "8                       Regulatory Analysis     0.031060\n",
       "9                     Development Economics     0.027822\n",
       "10               Spending, Deficits, & Debt     0.027345\n",
       "11                  Regulatory Accumulation     0.026811\n",
       "12                Government Accountability     0.019214\n",
       "13                                    Taxes     0.018034\n",
       "14            State and Local Budget Reform     0.016454\n",
       "15              State and Local Regulations     0.015459\n",
       "16                     Financial Regulation     0.014394\n",
       "17                  Workforce Participation     0.014134\n",
       "18     Energy and Environmental Regulations     0.012615\n",
       "19                    Trade and Immigration     0.012492\n",
       "20                           Budget Process     0.010951\n",
       "21             Study of American Capitalism     0.010662\n",
       "22                          Monetary Policy     0.009799\n",
       "23                         Economic History     0.009051\n",
       "24                        Barriers to Entry     0.008577\n",
       "25              Food and Health Regulations     0.008462\n",
       "26                              Puerto Rico     0.008214\n",
       "27                        Technology Policy     0.007377\n",
       "28                               Innovation     0.006928\n",
       "29               Spending and Budget Reform     0.006685\n",
       "..                                      ...          ...\n",
       "61                                 Medicaid     0.002084\n",
       "62                    Healthcare Favoritism     0.002053\n",
       "63                           Online Privacy     0.001900\n",
       "64                            Cybersecurity     0.001887\n",
       "65                        Economic Mobility     0.001872\n",
       "66                       Immigration Reform     0.001766\n",
       "67                     Midnight Regulations     0.001737\n",
       "68               State and Local Tax Policy     0.001693\n",
       "69  Department of Health and Human Services     0.001657\n",
       "70                           Net Neutrality     0.001643\n",
       "71                    Securities Regulation     0.001572\n",
       "72               Regulatory Impact Analysis     0.001522\n",
       "73                                 Medicare     0.001511\n",
       "74                     Economics of Culture     0.001474\n",
       "75                     Department of Energy     0.001411\n",
       "76                 Transportation Financing     0.001370\n",
       "77                      Financial Stability     0.001322\n",
       "78                        Income Inequality     0.001168\n",
       "79                           Monetary Rules     0.001120\n",
       "80                         Education Policy     0.001022\n",
       "81      Copyright and Intellectual Property     0.001019\n",
       "82                          Sharing Economy     0.000998\n",
       "83                          Spectrum Reform     0.000942\n",
       "84                         Internet Freedom     0.000930\n",
       "85                 Global Financial Markets     0.000918\n",
       "86                                  FinTech     0.000914\n",
       "87                       Export-Import Bank     0.000597\n",
       "88                         Balance of Trade     0.000555\n",
       "89           Social Security Administration     0.000399\n",
       "90                                  Bitcoin     0.000393\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_doc.txt','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tf_test =  tfv.transform([text]) \n",
    "text_pred = gscv_lr_tf2.predict_proba(tf_test)\n",
    "\n",
    "test_df = pd.DataFrame(columns=['label','probability'])\n",
    "test_df['label'] = le.inverse_transform(text_pred[0].all())[0]\n",
    "test_df['probability'] = text_pred[0]\n",
    "test_df.sort_values('probability', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "It guessed the tag used correctly! It also provided some other suggestions for strong contenders that could help somebody trying to tag this document provide additional, useful tags.\n",
    "\n",
    "Of course, performance on one document is not a very good test of a model. For that we look to our validation data that was set aside in our fourth cell. I will now run tests on all model using that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [499, 91]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d2a4debcb6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgscv_lr_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_tfv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \"\"\"\n\u001b[1;32m   1608\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1609\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [499, 91]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "ypred = gscv_lr_tf2.predict_proba(xvalid_tfv)\n",
    "print(log_loss(ypred[0], yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3point6]",
   "language": "python",
   "name": "conda-env-python3point6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
