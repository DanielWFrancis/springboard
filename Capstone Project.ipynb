{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Predicting Tags on a Corpus of Research Papers\n",
    "\n",
    "The Mercatus Center at George Mason University is a university research center that provides economic research and education. Mercatus employs a fairly large staff of media and policy outreach professionals who must digest and communicate economic findings to media outlets and political staffers in Washington, DC. \n",
    "\n",
    "Part of this outreach is the maintainance of a website that provides full access to all research papers, testimonials, opinion editorials, summaries, and other outputs. These publications are given tags that allow users to easily find related publications. So far, Mercatus has assigned tags somewhat haphazardly, entrusting junior staffers to choose tags with a preference for preexisting labels. \n",
    "\n",
    "The goal of this project is to clean the current corpus of tagged documents and then, using machine learning techniques available from the scikit-learn Python library, produce a machine learning model that can predict the most relevant tags for a new document. \n",
    "\n",
    "I have written a scraper that has crawled the Mercatus website for documents and saved each one, along with some useful metadata. The following notebook preprocesses this data and then fits a machine learning model that can be pickled and used later to produce actionable tag suggestions for Mercatus outreach staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First I use sklearn.dataset's `load_files()` method to load a classified dataset - a directory whose subdirectories' names are label names, and whose subdirectories' contents are the text files for that label. \n",
    "\n",
    "I then initialize and fill a dataframe from attributes of the load_files object, and drop rows which contain missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 1630 unique documents, with 3.198159509202454 labels per document, for a total of 5213 documents spread over 130 labels.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "trainer = load_files('data')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['filename'] = trainer.filenames\n",
    "df['text'] = [open(f, 'r').read() for f in trainer.filenames]\n",
    "df['label'] = [trainer.target_names[x] for x in trainer.target]\n",
    "df['author'] = [x.split('__')[0].split('--')[0].split('/')[-1] for x in df['filename']]\n",
    "df['date'] = pd.to_datetime([x.split('__')[-1].split('-')[-1].split('.txt')[0].strip() for x in df['filename']], \n",
    "                            errors='coerce')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(f\"Our dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"{len(df)/len(df.groupby('text').count())} labels per document, for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our dataset contains a large number of labels, and some have not been used in many years, while others have only been used a few times. \n",
    "\n",
    "The following code converts the `date` column to datetime objects, then groups by `label` and `date` to find only labels that have been active in the past four years (1460 days). \n",
    "\n",
    "We then remove labels that contain less than six documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our refined dataset contains 1615 unique documents, with for a total of 4894 documents spread over 91 labels.\n"
     ]
    }
   ],
   "source": [
    "df['date'] = df['date'].astype(np.int64) // 10 ** 9\n",
    "df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "\n",
    "df_active_labels = df[['label','date']].groupby('label').max().sort_values('date',ascending=True).reset_index()\n",
    "df_active_labels = df_active_labels[df_active_labels.date > datetime.date.today() - datetime.timedelta(1460)]\n",
    "df = df[df['label'].isin(df_active_labels.label)]\n",
    "\n",
    "df_large_categories = df.groupby('label').count()[df.groupby('label').count()['filename'] > 5].reset_index()[['label','filename']]\n",
    "df = df[df['label'].isin(df_large_categories.label)]\n",
    "\n",
    "print(f\"Our refined dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start working on our data. First we encode our labels and split our training and validation documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df.label)\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df.text.tolist(), y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we initialize two functions, one for stemming tokens using the `SnowballStemmer`, and the second for actually tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    tokens = [i for i in tokens if all(j.isalpha() or j in string.punctuation for j in i)]\n",
    "    tokens = [i for i in tokens if '/' not in i]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I initialize two vectorizers - a CountVectorizer and TfidfVectorizer, both of which cap the 1-gram, 2-gram, and 3-gram features at 200,000 and impose a minimum document frequency of three and a maximum document frequency of 70% of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "vect.fit(list(xtrain) + list(xvalid))\n",
    "o_xtrain_cv = vect.transform(xtrain)\n",
    "o_xvalid_cv = vect.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "o_xtrain_tfv =  tfv.transform(xtrain) \n",
    "o_xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model tests\n",
    "\n",
    "The goal is to provide users with the most relevant tags for a particular document. To accomplish this, I use `GridSearchCV` to tune hyperparameters for a variety of models, and then choose the best one. These models include `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, `KNeighborsClassifier`, and `LinearSVC`. These models will be tried using both tf-idf and count vectorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Testing LogisticRegression on tf-idf vectorized features.\n",
    "\n",
    "lr_tf = LogisticRegression()\n",
    "params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_lr_tf = GridSearchCV(lr_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_lr_tf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# Display LogisticRegression/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_tf.best_score_} found with the following parameters:',\n",
    "      gscv_lr_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing LogisticRegression on count vectorized features.\n",
    "\n",
    "lr_cv = LogisticRegression()\n",
    "params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_lr_cv = GridSearchCV(lr_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_lr_cv.fit(xtrain_cv, ytrain)\n",
    "\n",
    "# Display LogisticRegression/countvectorizer results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_cv.best_score_} found with the following parameters:',\n",
    "      gscv_lr_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Testing DecisionTreeClassifier on tf-idf vectorized features.\n",
    "\n",
    "dtc_tf = DecisionTreeClassifier()\n",
    "params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_dtc_tf = GridSearchCV(dtc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_dtc_tf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Testing DecisionTreeClassifier on count vectorized features.\n",
    "\n",
    "dtc_cv = DecisionTreeClassifier()\n",
    "params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_dtc_cv = GridSearchCV(dtc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_dtc_cv.fit(xtrain_cv, ytrain)\n",
    "\n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Testing RandomForestClassifier on tf-idf vectorized features.\n",
    "\n",
    "rfc_tf = RandomForestClassifier()\n",
    "params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_rfc_tf = GridSearchCV(rfc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_rfc_tf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "# Display RandomForestClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Testing RandomForestClassifier on count vectorized features.\n",
    "\n",
    "rfc_cv = RandomForestClassifier()\n",
    "params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "# Initialize and fit.\n",
    "\n",
    "gscv_rfc_cv = GridSearchCV(rfc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "gscv_rfc_cv.fit(xtrain_cv, ytrain)\n",
    "\n",
    "# Display RandomForestClassifier/count results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As the test results show, LogisticRegression on the tf-idf vectorized data returns the most reliable results. *write more when more results are available*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure out how to work this example in* - Just to satiate my curiosity, I ran test on a document that Mercatus has recently published that is not in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('test_doc.txt','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tf_test =  tfv.transform([text]) \n",
    "text_pred = rfc.predict_proba(tf_test)\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(columns=['label','probability'])\n",
    "test_df['label'] = le.inverse_transform(predictions_tf_clf[0].all())[0]\n",
    "test_df['probability'] = text_pred[0]\n",
    "test_df.sort_values('probability', ascending=False).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3point6]",
   "language": "python",
   "name": "conda-env-python3point6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
