{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Capstone Project - Predicting Tags on a Corpus of Research Papers\n",
    "\n",
    "The Mercatus Center at George Mason University is a university research center that provides economic research and education. Mercatus employs a fairly large staff of media and policy outreach professionals who must digest and communicate economic findings to media outlets and political staffers in Washington, DC. \n",
    "\n",
    "Part of this outreach is the maintainance of a website that provides full access to all research papers, testimonials, opinion editorials, summaries, and other outputs. These publications are given tags that allow users to easily find related publications. So far, Mercatus has assigned tags somewhat haphazardly, entrusting junior staffers to choose tags with a preference for preexisting labels. \n",
    "\n",
    "The goal of this project is to clean the current corpus of tagged documents and then, using machine learning techniques available from the scikit-learn Python library, produce a machine learning model that can predict the most relevant tags for a new document. \n",
    "\n",
    "I have written a scraper that has crawled the Mercatus website for documents and saved each one, along with some useful metadata. The following notebook preprocesses this data and then fits a machine learning model that can be pickled and used later to produce actionable tag suggestions for Mercatus outreach staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# This variable sets a master value for all models in this\n",
    "# notebook; individual models can be set differently within\n",
    "# their cells.\n",
    "\n",
    "use_pickles_master = True\n",
    "\n",
    "# initialize array for capturing all models\n",
    "\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First I use sklearn.dataset's `load_files()` method to load a classified dataset - a directory whose subdirectories' names are label names, and whose subdirectories' contents are the text files for that label. \n",
    "\n",
    "I then initialize and fill a dataframe from attributes of the load_files object, and drop rows which contain missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 1901 unique documents, with 2.850078905839032 labels per document, for a total of 5418 documents spread over 136 labels.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "trainer = load_files('new_data')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['filename'] = trainer.filenames\n",
    "df['text'] = [' '.join(open(f, 'r').read().split()) for f in trainer.filenames]\n",
    "df['label'] = [trainer.target_names[x] for x in trainer.target]\n",
    "df['author'] = [x.split('__')[0].split('--')[0].split('/')[-1] for x in df['filename']]\n",
    "df['date'] = pd.to_datetime([x.split('__')[-1].split('-')[-1].split('.txt')[0].strip() for x in df['filename']], \n",
    "                            errors='coerce')\n",
    "\n",
    "# Find documents with less than 250 characters and remove them\n",
    "df = df[pd.Series([len(x) for x in df.text]) > 250]\n",
    "\n",
    "# remove documents in NA values - sometimes dates got cut off,\n",
    "# which causes problems in the next cell\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Our dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"{len(df)/len(df.groupby('text').count())} labels per document, for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our dataset contains a large number of labels, and some have not been used in many years, while others have only been used a few times. \n",
    "\n",
    "The following code converts the `date` column to datetime objects, then groups by `label` and `date` to find only labels that have been active in the past four years (1460 days). \n",
    "\n",
    "We then remove labels that contain less than six documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our refined dataset contains 1870 unique documents, with for a total of 4946 documents spread over 90 labels.\n"
     ]
    }
   ],
   "source": [
    "df['date'] = df['date'].astype(np.int64) // 10 ** 9\n",
    "df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "\n",
    "df_active_labels = df[['label','date']].groupby('label').max().sort_values('date',ascending=True).reset_index()\n",
    "df_active_labels = df_active_labels[df_active_labels.date > datetime.date.today() - datetime.timedelta(1460)]\n",
    "df = df[df['label'].isin(df_active_labels.label)]\n",
    "\n",
    "df_large_categories = df.groupby('label').count()[df.groupby('label').count()['filename'] > 5].reset_index()[['label','filename']]\n",
    "df = df[df['label'].isin(df_large_categories.label)]\n",
    "\n",
    "print(f\"Our refined dataset contains {len(df.groupby('text').count())} unique documents, with\",\n",
    "     f\"for a total of\",\n",
    "     f\"{len(df)} documents spread over {len(df.groupby('label').count())} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can start working on our data. First we encode our labels and split our training and validation documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df.label)\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df.text.tolist(), y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we initialize two functions, one for stemming tokens using the `SnowballStemmer`, and the second for actually tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [i for i in tokens if i not in string.punctuation]\n",
    "    tokens = [i for i in tokens if all(j.isalpha() or j in string.punctuation for j in i)]\n",
    "    tokens = [i for i in tokens if '/' not in i]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I initialize two vectorizers - a CountVectorizer and TfidfVectorizer, both of which cap the 1-gram and 2-gram features at 200,000 and impose a minimum document frequency of three and a maximum document frequency of 70% of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    cv = pickle.load(open(\"countvectorizer.pkl\", \"rb\"))\n",
    "    xtrain_cv = pickle.load(open(\"xtrain_cv.pkl\", \"rb\"))\n",
    "    xvalid_cv = pickle.load(open(\"xvalid_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    cv = CountVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "    cv.fit(list(set(xtrain)) + list(set(xvalid)))\n",
    "    xtrain_cv = cv.transform(xtrain)\n",
    "    xvalid_cv = cv.transform(xvalid)\n",
    "\n",
    "    pickle.dump(xtrain_cv, open(\"xtrain_cv.pkl\",\"wb\"))\n",
    "    pickle.dump(xvalid_cv, open(\"xvalid_cv.pkl\",\"wb\"))\n",
    "    pickle.dump(cv, open(\"countvectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    tfv = pickle.load(open(\"tfidfvectorizer.pkl\", \"rb\"))\n",
    "    xtrain_tfv = pickle.load(open(\"xtrain_tfv.pkl\", \"rb\"))\n",
    "    xvalid_tfv = pickle.load(open(\"xvalid_tfv.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    tfv = TfidfVectorizer(min_df=3, max_df=0.7, max_features=200000, tokenizer=tokenize,\n",
    "                strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = 'english')\n",
    "\n",
    "    tfv.fit(list(xtrain) + list(xvalid))\n",
    "    xtrain_tfv =  tfv.transform(xtrain) \n",
    "    xvalid_tfv = tfv.transform(xvalid)\n",
    "\n",
    "    pickle.dump(xtrain_tfv, open(\"xtrain_tfv.pkl\",\"wb\"))\n",
    "    pickle.dump(xvalid_tfv, open(\"xvalid_tfv.pkl\",\"wb\"))\n",
    "    pickle.dump(tfv, open(\"tfidfvectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Tests: First Pass\n",
    "\n",
    "The goal is to provide users with the most relevant tags for a particular document. To accomplish this, I use `GridSearchCV` to tune hyperparameters for a variety of models, and then choose the best one. These models include `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, `KNeighborsClassifier`, and `LinearSVC`. These models will be tried using both tf-idf and count vectorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### LogisticRegression\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -3.3094250726587253 found with the following parameters: {'C': 5, 'dual': False, 'n_jobs': -1, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Testing LogisticRegression on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_tf = pickle.load(open(\"gscv_lr_tf.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    lr_tf = LogisticRegression()\n",
    "    params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_tf = GridSearchCV(lr_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_tf.fit(xtrain_tfv, ytrain)\n",
    "\n",
    "    pickle.dump(gscv_lr_tf, open(\"gscv_lr_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_lr_tf])\n",
    "    \n",
    "# Display LogisticRegression/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_tf.best_score_} found with the following parameters:',\n",
    "      gscv_lr_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -9.599575059251444 found with the following parameters: {'C': 0.5, 'dual': False, 'n_jobs': -1, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# # Testing LogisticRegression on count vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_cv = pickle.load(open(\"gscv_lr_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    lr_cv = LogisticRegression()\n",
    "    params = {'C':[0.5, 1, 5], 'dual':[False, True], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_cv = GridSearchCV(lr_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_lr_cv, open(\"gscv_lr_cv.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_lr_cv])\n",
    "       \n",
    "# Display LogisticRegression/countvectorizer results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_cv.best_score_} found with the following parameters:',\n",
    "      gscv_lr_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### DecisionTreeClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -30.866439479022038 found with the following parameters: {'criterion': 'gini', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Testing DecisionTreeClassifier on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_dtc_tf = pickle.load(open(\"gscv_dtc_tf.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    dtc_tf = DecisionTreeClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_dtc_tf = GridSearchCV(dtc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_dtc_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_dtc_tf, open(\"gscv_dtc_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_dtc_tf])\n",
    "       \n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -30.50875466565311 found with the following parameters: {'criterion': 'entropy', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Testing DecisionTreeClassifier on count vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_dtc_cv = pickle.load(open(\"gscv_dtc_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    dtc_cv = DecisionTreeClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_dtc_cv = GridSearchCV(dtc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_dtc_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_dtc_cv, open(\"gscv_dtc_cv.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_dtc_cv])\n",
    "       \n",
    "# Display DecisionTreeClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_dtc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_dtc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RandomForestClassifier\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -16.008716214283595 found with the following parameters: {'criterion': 'gini', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Testing RandomForestClassifier on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_rfc_tf = pickle.load(open(\"gscv_rfc_tf.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    rfc_tf = RandomForestClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_rfc_tf = GridSearchCV(rfc_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_rfc_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_rfc_tf, open(\"gscv_rfc_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_rfc_tf])\n",
    "       \n",
    "# Display RandomForestClassifier/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_tf.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -15.776711349094573 found with the following parameters: {'criterion': 'entropy', 'min_samples_split': 6, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "# Testing RandomForestClassifier on count vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_rfc_cv = pickle.load(open(\"gscv_rfc_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    rfc_cv = RandomForestClassifier()\n",
    "    params = {'criterion':['gini','entropy'], 'min_samples_split':[2,4,6], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_rfc_cv = GridSearchCV(rfc_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_rfc_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_rfc_cv, open(\"gscv_rfc_cv.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_rfc_cv])\n",
    "       \n",
    "# Display RandomForestClassifier/count results.\n",
    "\n",
    "print(f'Best score of {gscv_rfc_cv.best_score_} found with the following parameters:',\n",
    "      gscv_rfc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Tests for tf-idf and count vectorizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -4.02127379671145 found with the following parameters: {'alpha': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Testing NaiveBayes on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_mnb_tf = pickle.load(open(\"gscv_mnb_tf.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    mnb_tf = MultinomialNB()\n",
    "    params = {'alpha':[0.01, 1, 3]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_mnb_tf = GridSearchCV(mnb_tf, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_mnb_tf.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_mnb_tf, open(\"gscv_mnb_tf.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_mnb_tf])\n",
    "       \n",
    "# Display NaiveBayes/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_mnb_tf.best_score_} found with the following parameters:',\n",
    "      gscv_mnb_tf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -28.09564229668772 found with the following parameters: {'alpha': 3}\n"
     ]
    }
   ],
   "source": [
    "# Testing NaiveBayes on count vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_mnb_cv = pickle.load(open(\"gscv_mnb_cv.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    mnb_cv = MultinomialNB()\n",
    "    params = {'alpha':[0.01, 1, 3]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_mnb_cv = GridSearchCV(mnb_cv, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_mnb_cv.fit(xtrain_cv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_mnb_cv, open(\"gscv_mnb_cv.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_mnb_cv])\n",
    "       \n",
    "# Display NaiveBayes/count results.\n",
    "\n",
    "print(f'Best score of {gscv_mnb_cv.best_score_} found with the following parameters:',\n",
    "      gscv_mnb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Tests: A Second Pass\n",
    "\n",
    "As the test results show, LogisticRegression on the tf-idf vectorized data returns the most reliable results. A further dive into the the GridSearchCV test results shows a pattern of improving test scores as C-values increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.5, mean_test_score=-3.608504471830679\n",
      "For C=0.5, mean_test_score=-3.608512043830933\n",
      "For C=1.0, mean_test_score=-3.420553423569543\n",
      "For C=1.0, mean_test_score=-3.42056045461182\n",
      "For C=5.0, mean_test_score=-3.3094250726587253\n",
      "For C=5.0, mean_test_score=-3.309442896912445\n"
     ]
    }
   ],
   "source": [
    "for count, c in enumerate([0.5, 0.5, 1.0, 1.0, 5.0, 5.0]):\n",
    "    print(f\"For C={c}, mean_test_score={list(gscv_lr_tf.cv_results_['mean_test_score'])[count]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Exploring higher C-values makes sense, given the trend we are seeing here. Each `C` value appears twice as each value was tested along with a `dual` value of `True` and `False`; the `dual` value does not seem to make much of a difference, so we'll drop it in our next test and only test further values of `C`. We will also test a few other parameters, namely `penalty` and `tol`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -2.8691197965860096 found with the following parameters: {'C': 5, 'n_jobs': -1, 'penalty': 'l1', 'random_state': 42, 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "# Second testing of LogisticRegression on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_lr_tf2 = pickle.load(open(\"gscv_lr_tf2.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    lr_tf2 = LogisticRegression()\n",
    "    params = {'penalty':['l1','l2'], 'C':[1, 5, 10], 'tol':[0.001, 0.0001, 0.00001], 'n_jobs':[-1], 'random_state':[42]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_lr_tf2 = GridSearchCV(lr_tf2, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_lr_tf2.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_lr_tf2, open(\"gscv_lr_tf2.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_lr_tf2])\n",
    "       \n",
    "# Display LogisticRegression/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_lr_tf2.best_score_} found with the following parameters:',\n",
    "      gscv_lr_tf2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Excellent! Reducing our log-loss to -2.87 is a big improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Similarly, our tf-idf-vectorized Multinomial Naive Bayes returned respectable results at its maximum `alpha` tested - 3 - so it makes sense to test it at higher alphas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score of -3.8707800902353537 found with the following parameters: {'alpha': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Testing NaiveBayes on tf-idf vectorized features.\n",
    "\n",
    "# should we generate a fresh model, or should we use pickles?\n",
    "# by default this is the master value set in cell 1; overwrite\n",
    "# it for a specific cell by specifying True or False.\n",
    "\n",
    "use_pickles = use_pickles_master\n",
    "\n",
    "if use_pickles:\n",
    "    gscv_mnb_tf2 = pickle.load(open(\"gscv_mnb_tf2.pkl\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "    mnb_tf2 = MultinomialNB()\n",
    "    params = {'alpha':[4, 5, 6, 7, 8, 9]}\n",
    "\n",
    "    # Initialize and fit.\n",
    "\n",
    "    gscv_mnb_tf2 = GridSearchCV(mnb_tf2, param_grid=params, scoring='neg_log_loss', n_jobs=-1)\n",
    "    gscv_mnb_tf2.fit(xtrain_tfv, ytrain)\n",
    "    \n",
    "    pickle.dump(gscv_mnb_tf2, open(\"gscv_mnb_tf2.pkl\",\"wb\"))\n",
    "\n",
    "\n",
    "# add model to the models array\n",
    "models.extend([gscv_mnb_tf2])\n",
    "       \n",
    "# Display NaiveBayes/tf-idf results.\n",
    "\n",
    "print(f'Best score of {gscv_mnb_tf2.best_score_} found with the following parameters:',\n",
    "      gscv_mnb_tf2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an improvement over the score at `alpha = 3`, but it's still not competitive with LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just to sate my curiosity, I used the `gscv_lr_tf2` model to run a test on a document that Mercatus has recently published that is not in the corpus, [this \"Economic Situation\" quarterly report](https://www.mercatus.org/publications/economic-situation-december-2017). It's useful to note that this document has only one tag on the website, \"Economics and Public Policy\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economics and Public Policy</td>\n",
       "      <td>0.121860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State and Local Regulations</td>\n",
       "      <td>0.075437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Federal Fiscal Policy</td>\n",
       "      <td>0.064857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Regulatory Report Card</td>\n",
       "      <td>0.064778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regulatory Accumulation</td>\n",
       "      <td>0.051593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label  probability\n",
       "0  Economics and Public Policy     0.121860\n",
       "1  State and Local Regulations     0.075437\n",
       "2        Federal Fiscal Policy     0.064857\n",
       "3       Regulatory Report Card     0.064778\n",
       "4      Regulatory Accumulation     0.051593"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test_doc.txt','r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tf_test =  tfv.transform([text]) \n",
    "text_pred = gscv_lr_tf2.predict_proba(tf_test)\n",
    "\n",
    "test_df = pd.DataFrame(columns=['label','probability'])\n",
    "test_df['label'] = le.inverse_transform(text_pred[0].all())[0]\n",
    "test_df['probability'] = text_pred[0][:90]\n",
    "test_df.sort_values('probability', ascending=False).reset_index(drop=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "It guessed the tag used correctly! It also provided some other suggestions for strong contender tags that could help somebody trying to tag this document more thoroughly.\n",
    "\n",
    "Of course, performance on one document is not a very good test of a model. For that we look to our validation data that was set aside in our fourth cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [499, 495]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d1345e905f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgscv_lr_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_tfv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Log-loss on test set is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1607\u001b[0m     \"\"\"\n\u001b[1;32m   1608\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1609\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3point6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [499, 495]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# generate prediction array and score accuracy using log_loss\n",
    "\n",
    "ypred = gscv_lr_tf2.predict_proba(xvalid_tfv)\n",
    "print('Log-loss on test set is', log_loss(yvalid, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Excellent! Our test results for the second pass of tf-idf-vectorized Logistic Regression are even better than our validation results above. This model is ready for production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3point6]",
   "language": "python",
   "name": "conda-env-python3point6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
